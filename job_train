#!/usr/bin/env bash

#SBATCH --partition=gpu-long  # Partition to submit to
#SBATCH --job-name=benchmark
#SBATCH --ntasks-per-node=1
#SBATCH --constraint="[a40|a100]"
#SBATCH --gres=gpu:1
# Runtime in D-HH:MM
#SBATCH --time=5-20:00:00
#SBATCH --mem=120G    
#SBATCH --output=./sbatch_training/slurm5.out # STDOUT

echo "SLURM_JOBID: " $SLURM_JOBID
echo "SLURM_ARRAY_TASK_ID: " $SLURM_ARRAY_TASK_ID
echo "SLURM_ARRAY_JOB_ID: " $SLURM_ARRAY_JOB_ID

base_dir=/project/pi_ekalogerakis_umass_edu/pgoyal
batch_size=8

python -u train.py --base_dir=$base_dir --enable_flash --base_output batch_training --batch_size=$batch_size